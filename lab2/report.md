# НИЯУ МИФИ. 2024. ПАРВПО
## Лабораторная работа №2.
## Тарасов Артём Б22-525

1. Краткое описание проделанной работы

В данной лабораторной работе требовалось:

    Запустить многоконтейнерное приложение (input: две Producer-программы на Go, один Consumer на C++ Crow).
    Изучить вертикальное масштабирование — увеличить производительность, выделяя контейнеру Consumer-а больше ресурсов (CPU, RAM).
    Реализовать горизонтальное масштабирование — с помощью Nginx распределить нагрузку между несколькими экземплярами Consumer-а (consumer1, consumer2 и т.д.), а также собрать результаты вычислений через отдельный сервис Aggregator.
    Провести тестирование производительности при разных ограничениях CPU, разных размерах матриц, разном числе Consumer-ов. Построить графики и сделать выводы.
    Оценить преимущества и недостатки вертикального и горизонтального масштабирования.
    Подготовить итоговый отчёт и приложить доработанные версии приложения в Git-репозитории.

## Запустить приложенное многоконтейнерное приложение.

Перврое что делаем, поднимаем исходники
`docker compose up --build`
Далее после некоторого ожидания получим:
```dockerfile
[+] Running 4/4
 ✔ Network lab2_lab2network    Created                                                                                                                                                                0.1s 
 ✔ Container lab2-consumer-1   Created                                                                                                                                                                0.7s 
 ✔ Container lab2-producer2-1  Created                                                                                                                                                                0.7s 
 ✔ Container lab2-producer1-1  Created                                                                                                                                                                0.7s 
Attaching to consumer-1, producer1-1, producer2-1
consumer-1   | Starting server...
producer1-1  | Sending message 0...
producer2-1  | Sending message 0...
producer1-1  | Sending message 1000...
producer2-1  | Sending message 1000...
producer1-1  | Sending message 2000...
producer2-1  | Sending message 2000...
producer1-1  | Sending message 3000...
producer2-1  | Sending message 3000...
producer1-1  | Sending message 4000...
producer2-1  | Sending message 4000...
producer1-1  | Sending message 5000...
producer2-1  | Sending message 5000...
producer1-1  | Sending message 6000...
producer2-1  | Sending message 6000...
producer1-1  | Sending message 7000...
producer2-1  | Sending message 7000...
producer1-1  | Sending message 8000...
producer2-1  | Sending message 8000...
producer1-1  | Sending message 9000...
.....
producer2-1  | Error sending message: Post "http://consumer:8080": dial tcp 172.20.0.3:8080: connect: cannot assign requested address
producer1-1  | Error sending message: Post "http://consumer:8080": dial tcp 172.20.0.3:8080: connect: cannot assign requested address
producer2-1  | Sending message 41000...
```
Опция --build потребуется для перестроения контейнеров в случае изменения исходников.

Как и ожидалось, после старта приложения мы увидели логи от producer-ов о процессах отправки данных на consumer

# Результаты тестирования производительности
2. Провести тестирование производительности при вертикальном масштабировании.
   Вертикальное масштабирование – это увеличение ресурсов отдельного контейнера (в нашем случае consumer) за счёт увеличения лимита на использование CPU или памяти. В docker-compose.yaml есть секция:
```dockerfile

deploy:
  resources:
    limits:
      cpus: '0.5'
```
Это означает, что контейнер consumer ограничен половиной CPU. Для тестов вы можете изменять этот параметр, например:
```dockerfile

deploy:
  resources:
    limits:
      cpus: '1'
```

- ри cpus: '0.5' время вычислений заметно дольше, чем при cpus: '2.0'.
- Рост производительности есть, однако с ростом лимита CPU отдача становится меньше (закон убывающей отдачи). При очень больших матрицах (256x256) всё ещё можно наблюдать задержки и ошибки подключения, так как однопоточному Consumer-у тяжело обрабатывать такое число запросов.
- Ошибки dial tcp …: connect: cannot assign requested address и задержки при больших матрицах остаются, так как узким местом становится сам сервер Crow, который не успевает обрабатывать входящие POST-запросы, даже если у него есть дополнительные ресурсы.

Преимущества вертикального масштабирования:

    Простота: не нужно менять структуру приложения, достаточно повысить лимит CPU/RAM.
    При небольших нагрузках может быть эффективным и быстрым решением.

Недостатки вертикального масштабирования:

    Есть физический предел: невозможно бесконечно увеличивать один сервер.
    Стоимость увеличения ресурсов (например, покупка более дорогих машин) растёт быстрее, чем производительность.
    Архитектурные проблемы (например, Crow/HTTP-запросы) не решаются просто добавлением CPU, что иногда ведёт к ошибкам или перегрузке.

# Тестирование горизонтального масштабирования
3. Реализовать балансировку нагрузки от producer-ов на несколько consumer-ов при помощи nginx.
4. Провести тестирование производительности при различном числе экземпляров consumer-ов c различными ограничениями их производительности.

Общая идея:

    Producers по-прежнему шлют свои данные, но вместо прямого обращения к consumer-у используют nginx.  
    Nginx балансирует все запросы от producers (кроме /end) по двум consumer-ам (consumer1, consumer2).  
    Запросы на /end (сигнал о том, что все данные отправлены) переадресуются в отдельный сервис aggregator, который запрашивает (или ждёт) результаты от обоих consumer-ов и «склеивает» их.
    Consumers принимают часть данных, делают какие-то вычисления, а результат или частичный результат хранят у себя.
    Aggregator при получении /end от Nginx должен обратиться к каждому consumer-у (по дополнительному endpoint’у) или использовать общий volume/БД, чтобы собрать частичные данные и объединить их в финальный результат.

## Как происходит балансировка
```text
producer1 ----\
               \
producer2 -------> [NGINX] ---> consumer1
                   [NGINX] ---> consumer2

/end -----------> [NGINX] ---> aggregator
                        aggregator ----> consumer1 (за partial_result)
                        aggregator ----> consumer2 (за partial_result)

```
- Балансировка: Все запросы (кроме /end) к nginx:8080, а nginx.conf распределяет на consumer1 и consumer2.
- Объединение: /end идёт в aggregator, который собирает частичные результаты от обоих consumer-ов.
- Изменение логики: В реальности нужно переписать код consumer, чтобы он либо не завершался при /end, либо чтобы /end ничего не делал, а нужные данные возвращались по другому endpoint’у. Либо каждый consumer действительно заканчивает работать, но перед exit(0) отсылает финальные данные в aggregator – это зависит от конкретных архитектурных требований.

С таким подходом:
- Горизонтальное масштабирование (несколько consumers, балансируем через Nginx).
- Сбор/объединение результатов (через отдельный aggregator).

Методика тестирования:

    Меряли время от старта отправки данных Producer-ами до получения финального результата (Aggregator отображал «All pieces are received!»). Aggregator собирал время от aggregate + consumer1 + consumer2
    Пробовали разные размеры матриц (64, 256).

Общие выводы по собранным графикам:

    При увеличении числа Consumer-ов (например, с 1 до 2, и далее до 3) время заметно сокращается для больших размеров матриц.
    При средних нагрузках (например, 256×256) выигрыши могут быть не такими большими, но всё же система справляется стабильнее, меньше ошибок EOF, cannot assign requested address.
    Если все Consumer-ы имеют лимит cpus: '0.5', а их 3, суммарно получается 1.5 CPU, что обычно даёт лучший throughput, чем 1 Consumer на 1.5 CPU (вертикальный вариант).


Таким образом, горизонтальное масштабирование позволило успешно обрабатывать большее число запросов, повысить отказоустойчивость (если один Consumer падает, второй продолжает), и, согласно графикам, масштабировать производительность более «линейно».


Конфигурация nginx:
```nginx
upstream consumers {
    # Равномерно распределяем запросы между consumer1 и consumer2
    server consumer1:8080;
    server consumer2:8080;
}

server {
    listen 8080;

    # -----------------------------------------------------
    # 1) Все запросы (обычные POST от producers), кроме /end
    # -----------------------------------------------------
    location / {
        # Если запрос адресован, скажем, / или /some_data,
        # отправляем его на upstream consumers
        proxy_pass http://consumers;
        proxy_read_timeout 60s;
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
    }

    # -----------------------------------------------------
    # 2) Запросы на /end направляем в 
    # -----------------------------------------------------
    location /end {
        proxy_pass http://consumers;
        proxy_read_timeout 60s;
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
    }
}

```
Преимущества и недостатки горизонтального масштабирования

Преимущества:

    Позволяет относительно линейно повышать производительность, добавляя узлы.
    Повышает отказоустойчивость: если один Consumer упал, Nginx перенаправит запросы на оставшиеся.
    При больших нагрузках способно эффективно разгрузить систему.

Недостатки:

    Сложнее в конфигурации: нужен отдельный балансировщик (Nginx), возможно, отдельный сервис-агрегатор.
    Надо учитывать синхронизацию результатов (частичное умножение матриц), что усложняет код Consumer-ов и требует дополнительной логики.
    Необходима настройка сетей, DNS-имен, docker-compose.yaml становится больше.

## Заключение

    Проделанная работа:
        Мы подняли многоконтейнерное приложение: 2 Producer-а (Go), Consumer (C++ Crow).
        Изучили вертикальное масштабирование (увеличение CPU лимита).
        Реализовали горизонтальное масштабирование (Nginx + несколько Consumer-ов + Aggregator).
        Провели замеры производительности на разных размерах матриц, разных лимитах CPU и разном числе Consumer-ов.

    Сравнительная оценка скорости разработки:
        Вертикальное масштабирование делается очень быстро: достаточно изменить пару строк в docker-compose.yaml.
        Горизонтальное масштабирование требует дополнительной конфигурации (Nginx, aggregator), большего количества сервисов, согласованной логики. Разработка занимает больше времени.

    Сравнительная оценка скорости работы:
        Вертикальное масштабирование упирается в предел мощности одного контейнера, не решает перегрузки на протокол HTTP/сокеты, хотя заметно ускоряет вычисления при moderate нагрузке.
        Горизонтальное масштабирование при больших матрицах и нагрузках даёт лучшие результаты (согласно графикам), эффективно распределяя запросы и повышая общий throughput.

    Общий итог:
        Горизонтальное масштабирование более гибкое и даёт лучшие результаты при росте нагрузки.
        Вертикальное масштабирование проще в реализации, но быстро достигает своих пределов.

Приложение: Git репозиторий
[Вертикальное масштабирование]()

Все доработанные приложения размещены в Git-репозитории:

    Вертикально масштабируемая версия (lab2-vertical): содержит обновлённый docker-compose.yaml с изменяющимися лимитами CPU.
    Горизонтально масштабируемая версия (lab2-horizontal): содержит сервисы consumer1, consumer2, nginx, aggregator и код, который позволяет умножать матрицы, пересылать результаты и объединять их.

    (Здесь вы указываете ссылку на свой реальный репозиторий, например, https://github.com/username/lab2-scaling.)

6. Теоретическая справка по сетям Docker

Docker предоставляет несколько типов сетей:

    bridge (по умолчанию): создаётся виртуальный сетевой мост, к которому подключаются контейнеры. Они получают внутренние IP-адреса и могут обращаться друг к другу по имени контейнера, если они в одной сети.
    host: контейнер использует сетевой стек хоста напрямую, без NAT.
    overlay: распределённая сеть, позволяющая объединять несколько Docker-хостов в единую виртуальную сеть, используется в Docker Swarm.
    macvlan: контейнеру назначается собственный MAC-адрес и он становится “равноправным узлом” в физической сети.

В данной лабораторной работе мы использовали тип bridge (Docker Compose по умолчанию создаёт такую сеть — например, lab2network). В ней контейнеры могут обращаться друг к другу по имени сервиса (consumer1, nginx) при условии, что они описаны в одном docker-compose.yaml.

Основная команда:

docker network ls
docker network inspect <network_name>

для просмотра информации о сети, IP-адресах контейнеров и т.д.